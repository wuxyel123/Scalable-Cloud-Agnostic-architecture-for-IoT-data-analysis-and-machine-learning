\chapter{Methodology}
\label{cap:method}

\intro{In this chapter, we will present the methodology used to develop the proposed solution. The chapter is divided into two sections: architecture, and implementation. The architecture section will present the proposed solution's architecture for the chosen cloud providers, while the implementation section will detail the steps taken to implement the solution.
}\\


\section{Architecture}
\label{sec:architecture}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{architecture.png}
    \caption{Architecture of the proposed solution}
\end{figure}

The proposed solution's architecture consists of a composition of loosely coupled microservices that work together to collect, process, and store data. The architecture is divided into three layers: the data collection layer, the data storage layer and the data analysis layer. Each layer plays a specific role in the overall system, ensuring scalability, flexibility, and resilience.

\subsection{Data Collection Layer}

The data collection layer is responsible for gathering data from IoT devices and archived on-premise data. 

\subsection{Data Collection from IoT Devices}

The data collection from IoT devices utilize the MQTT protocol, a lightweight messaging protocol designed for devices with low bandwidth and high latency. MQTT uses a publish-subscribe model: devices send messages to a broker, which then distributes them to the subscribers. This protocol is popular in IoT development because it's simple, easy to implement, and reliable. Different quality of service (QoS) levels can be used to ensure an effective message delivery.

In our setup, numerous IoT devices can send data to the cloud. Each device is configured to publish data on a specific topic, which is managed by a cloud-based broker. The data containing a timestamp and key-value pairs representing sensor readings is sent in JSON format. Once the data reaches the broker, it is preprocessed before storage. This preprocessing step involves parsing the JSON data, extracting the timestamp and sensor values, and converting the data to CSV format. CSV was chosen for its simplicity, broad compatibility with data processing tools, and because it's less verbose than JSON. Finally, the preprocessed data is stored in object storage, where it can be accessed by the data processing pipeline.


\subsection{Data Collection from Archived Data}
To collect data from archived sources, files from on-premises systems are uploaded to the cloud using a straightforward script that leverages cloud's API. Since these files are already in CSV format no preprocessing is needed, so they are directly uploaded to object storage, making them readily accessible for the data processing pipeline. These files hold historical data that has been gathered over time across different scenarios.

\subsection{Data Storage Layer}

The data storage layer is designed to store raw and analyzed CSV data. It ensures that row data are available for the processing pipeline and that analyzed data are stored for future use. Raw data and analysis outputs are saved in different buckets to ensure a less complicated data management pipeline.
This layer can leverage on any Object Storage service offered by any provider since all of them are cost effective, scalable and redundat. 

\subsection{Data Analysis Layer}

The data analysis layer is dedicated to examining the collected data and generating meaningful insights. This layer comprises multiple services that retrieve preprocessed data from object storage to carry out various analytical tasks. The data is first downloaded from the object storage to a local storage leveraging the object storage APIs. This needs to be done since Object Storage services do not allow to access the content of the files directly.

Once the data is retrieved one or more analytics services run to analyze the data. These services can leverage simple statistical assessments as well as more advanced machine learning and deep learning techniques.
The analysis process extracts valuable insights that can be showed to the users of a certain application to enhance their experience. 
The analyzed data are then saved in the object storage again, but in a different bucket making them accessible for future use by other services or users.

\subsection{Cloud Server vs. SaaS Solutions}

The usage of a cloud server to host both the MQTT broker and the data analytics pipeline offers many advantages with respect to a SaaS solution.
A SaaS architectures can be convenient with respect to a IaaS one when talking about management and responsibilities. On the other hand, an IaaS based architecture allows for more flexibility and control of the underlying infrastructure. This can be especially useful in case that custom configurations and dependencies are needed for the data analytics pipeline.
A cloud server allows for better cost management with the only downside being the responsibility of managing the operating system and applying security patches. In contrast, SaaS solutions, are generally easier to set up and maintain but can also have higher costs over time due to higher subscription fees.
The cloud server approach is also inherently cloud-agnostic since every provider offers an IaaS service. This aspect ensures that users can select the cloud provider that best fits with their requirements without needing to change the architectural design of the solutions. With this solution, the MQTT broker and the data analytics pipeline are hosted on the same server but they function independently so the principles of microservices architectures are respected.
In summary, this architecture is robust, scalable, and flexible. Its cloud-agnostic nature allows for an easy implementation with any provider in the market. Furthermore, the cost-effective design can be particularly important for long-term projects. This approach not only meets the immediate operational needs but also positions organizations to adapt and evolve as their requirements change over time.

\section{Test Implementation}

In this section, the base implementation of the proposed solution is presented. The implementation is divided into two main components: the data collection component and the data analysis component. The data collection component is responsible for collecting data from IoT devices and archived data, while the data analysis component is responsible for processing and analyzing the collected data.

\subsection{Data Collection Component}

The data collection component is responsible for collecting data from IoT devices and archived data. This component consists of three main parts: the MQTT broker, the preprocessing pipeline and the data upload script.

\subsubsection{MQTT Broker}
MQTT broker is a lightweight messaging broker that implements the MQTT protocol. The broker is responsible for receiving messages from IoT devices and forwarding them to subscribers. The broker is hosted on a cloud server and is accessible to all IoT devices connected to the network. The broker is configured to use a specific topic for each device, ensuring that messages are delivered to the correct destination. 
The broker that we have chosen for the test implementation \href{https://www.emqx.io/}{EMQX}\footcite{site:emqx}, whose pros and cons have been discussed in \ref{emqx}.
Once the broker was been installed and configured in the cloud server, it was tested by connecting some simulated IoT devices which was then sending messages to the broker. The messages were successfully received by the broker and forwarded to the subscriber, demonstrating the broker's functionality.
The IoT devices used in the test were simulated using the \href{https://mqttx.app/}{MQTTX}\footcite{site:mqttx} client, analyzed in \ref{mqttx}. This MQTT client allows users to simulate IoT devices and publish messages to an MQTT broker. The devices were configured to publish messages to the broker using a specific topic, with each message containing a timestamp and a set of key-value pairs representing the data.

\subsubsection{Preprocessing pipeline}
The preprocessing pipeline is responsible for parsing the JSON data received from the MQTT broker, extracting the timestamp and key-value pairs, and converting the data to CSV format. The pipeline is implemented as a Python script that subscribes to the MQTT broker, receives messages, and preprocesses the data. The script uses the \href{https://pypi.org/project/paho-mqtt/}{Paho MQTT}\footcite{site:paho-mqtt} client library to connect to the broker and receive messages. Once the data is received, it is parsed, and the timestamp and key-value pairs are extracted. The data is then converted to CSV format and stored in object storage. The preprocessing pipeline ensures that the data is in a suitable format for further analysis and processing. The script was tested by connecting it to the MQTT broker and receiving messages from the simulated IoT devices. The messages were successfully parsed, and the data was converted to CSV format, demonstrating the pipeline's functionality.

\subsubsection{Data Upload Script}
The data upload script is responsible for uploading archived data from on-premises to the cloud. The script is implemented as a Python script that uses the cloud provider's API to upload files to object storage. The files contain historical data that have been collected over time in many different scenarios. The script reads the files from a local directory, uploads them to object storage, and makes them accessible to the data processing pipeline. In this scenario the files are already stored in CSV format, making them suitable for direct upload to object storage. The script was tested by uploading several sample files to object storage and verifying that the files were successfully uploaded to the right directory and accessible to the data processing pipeline. 


\subsection{Data Analysis Component}
The data analysis component is responsible for processing and analyzing the collected data. This component can consist of one or more data analytics scripts based on the requirements of the project. Each of the scripts needs to retrieve the correct data from the object storage, perform the required analysis, and store the results in a separate object storage bucket. In the case more than one file needs to be retrieved from storage, the scripts need to manage the data retrieval and processing in parallel to optimize performance and resource usage. The scripts can be implemented in any programming language that supports the required data processing and analysis tasks, such as Python or R. The choice of language depends on the specific requirements of the project and the availability of libraries and tools for data processing and analysis.
This component was tested by implementing a simple data analysis script that retrieves the preprocessed data from object storage, calculates simple statistics, and stores the results in a separate object storage bucket. The script was implemented in Python using various Python libraries such as Pandas and NumPy for data manipulation and analysis. The script was tested by retrieving the preprocessed data from object storage, calculating the statistics, and storing the results in a separate object storage bucket. The results were successfully stored, demonstrating the functionality of the data analysis component.
Once the analytics are uploaded they can be accessed via a web interface or a REST API, depending on the requirements of the project. The results can be visualized using various data visualization tools such as Matplotlib, Plotly, or Grafana, providing insights into the underlying patterns and trends in the collected data.